{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf8ddb4-693b-428d-8ffa-4a6fc940cb6a",
   "metadata": {},
   "source": [
    "# Diffusion \"Self-guidance\"\n",
    "\n",
    "https://dave.ml/selfguidance/\n",
    "\n",
    "**Use-case:** Method to move/resize objects, replace objects with new ones, change scene backgrounds of diffusion generated images\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "- Uses the attention maps and activations to steer the image generation\n",
    "  - both are from the attention layers of the diffusion model<sup>1</sup>\n",
    "  - these allow us to figure out objects, object position/size...\n",
    "- Using the above we figure out a value to add to the \"noise\" generated at each step of de-noising<sup>2</sup>\n",
    "  - since the noise is subtracted from the noisy image to produce a clearer image<sup>3</sup>\n",
    "  - this extra value effectively changes what the image produced will look like\n",
    "- Math on the \"self-guidance\" <sup>4</sup>\n",
    "  - Object position: Computed as center of mass of relevant attention channels\n",
    "  - Object size: Spatial sum of thresholded attention channel\n",
    "  - Object shape: Thresholded attention map itself\n",
    "  - Object appearance: Combination of thresholded attention and spatial activation maps\n",
    "\n",
    "<details>\n",
    "  <summary>Questions</summary>\n",
    "\n",
    "- Everything on the third bullet above \"object position...\", no idea what any of those things mean. Add notes? **medium- priority**\n",
    "- What's an attention map? **medium- priority**\n",
    "  - Perhaps do notes on the recurrent models chapter up to transformers to solidify the attention concept\n",
    "  - Then get this attention map info if still needed\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Context/Superscripts</summary>\n",
    "\n",
    "1. The original diffusion paper (Ho et al.) does not use attention layers, but the ones today (2024) do. Ex. Stable Diffusion\n",
    "2. If I forget this, just skim the diffusion folder notes\n",
    "3. Not really \"clearer\" as in there's a final form for the noisy image that's the same all the time. I just mean getting closer to the formed image by removing a bit more noise.\n",
    "4. I honestly have no idea what these things mean like \"center of mass\" of attention channels. I'm considering coming back to this after a firmer grasp on attention, then attention maps. Then see if I get closer to grokking this, since I need a better grasp on attention/recurrent NN fundamentals to advance on involving in AI research regardless of this paper\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6897ea-3cee-4264-b530-5e3bad1aa5b0",
   "metadata": {},
   "source": [
    "## Math\n",
    "\n",
    "### Typical diffusion (de-noise)\n",
    "\n",
    "Formula\n",
    "\n",
    "$$\n",
    "\\hat{\\epsilon}_t = (1 + s)\\epsilon_{\\theta}(z_t; t, y) - s\\epsilon_{\\theta}(z_t; t, \\emptyset)\n",
    "$$\n",
    "\n",
    "Overly simplistic pseudo-math:\n",
    "\n",
    "$$\n",
    "NoiseToRemove = (NoiseToRemove | TextPrompt) - (NoiseToRemove | NoTextPrompt) \n",
    "$$\n",
    "\n",
    "- $\\hat{\\epsilon}_t$ is noise to be removed at step t\n",
    "- $\\epsilon_{\\theta}$ is the function to predict the noise\n",
    "  - $\\epsilon_{\\theta}(z_t; t, y)$ predicts the noise given the conditioning info (from text prompt)\n",
    "  - $\\epsilon_{\\theta}(z_t; t, \\emptyset)$ predicts the noise given no conditioning\n",
    "- $z_t$ is the current noisy image\n",
    "- y is the conditioning (like a text prompt)\n",
    "- $\\emptyset$ is no conditioning (gen diffusion as if no text prompt is there<sup>1</sup>)\n",
    "\n",
    "### ...with self-guidance (de-noise)\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\hat{\\epsilon}_t = (1 + s)\\epsilon_{\\theta}(z_t; t, y) - s\\epsilon_{\\theta}(z_t; t, \\emptyset) + v\\sigma_t * \\nabla_{z_t} g(z_t; t, y)\n",
    "$$\n",
    "\n",
    "Overly simplistic pseudo-math:\n",
    "\n",
    "$$\n",
    "NoiseToRemove = ...OriginalStuff... + NewStuff\n",
    "$$\n",
    "\n",
    "$$\n",
    "NewStuff = TODO!\n",
    "$$\n",
    "\n",
    "- $v\\sigma_t * \\nabla_{z_t} g(z_t; t, y)$ is the only thing thats different (added)\n",
    "  - calculating this value for each step allows for _self-guidance_ (changing object size, object position, background etc)\n",
    "\n",
    "<details>\n",
    "  <summary>Questions</summary>\n",
    "\n",
    "- Explain all the variables in $v\\sigma_t * \\nabla_{z_t} g(z_t; t, y)$ and what they mean/do **medium- priority**\n",
    "- Why is the first question noise w/ text conditioning minus noise w/o text conditioning? **low+ priority**\n",
    "  - How subtracting nosie w/o text conditioning help with finding the correct noise amount to remove at that step?\n",
    "- In the first equation, why is the scalars (1+s) applied? **low- priority**\n",
    "- In the first equation, what exactly does the math function/equation for $\\epsilon_{\\theta}$ actually look like? **low+ priority**\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Context/Superscripts</summary>\n",
    "\n",
    "1. though doesn't quite mean \"empty\" string. perhaps digging into this will unveil what it is mathematically (low priority)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d0aac-b47d-40c7-aeaa-d3f9def30761",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Core changes:\n",
    "- Use Stable Diffusion (inherits `StableDiffusionXLPipeline` class)\n",
    "- Edit the `__call__` function of `StableDiffusionXLPipeline` by making a new class `SelfGuidanceStableDiffusionXLPipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032939a-e3ef-4ce1-b644-96129198bfdc",
   "metadata": {},
   "source": [
    "### Diagram\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Initialize Pipeline] --> B[Encode Prompt]\n",
    "    B --> C[Prepare Latents]\n",
    "    subgraph SDXL.__call__[\"SDXLPipeline.__call__()\"]\n",
    "        C --> D[Start Denoising Loop]\n",
    "        D --> E[Predict Noise]\n",
    "        E --> F[Apply Classifier-Free Guidance]\n",
    "        F --> G{Self-Guidance Active?}\n",
    "        G -->|No| H[Update Latents]\n",
    "        G -->|Yes| I[Compute & Apply Self-Guidance]\n",
    "        I --> H\n",
    "        H --> J{More Steps?}\n",
    "        J -->|Yes| E\n",
    "        J -->|No| K[Generate Image]\n",
    "    end\n",
    "    K --> L[Post-process & Return Image]\n",
    "\n",
    "    %% Linus-style comments positioned near relevant components\n",
    "    %% N1[Initialize: Set up your shit properly or get out.]\n",
    "    N2[The orange parts are only in SelfGuidanceSDXLPipeline.__call__ and not in SDXLPipeline.__call__]\n",
    "    %% N3[Self-Guidance: Optional. Use it right or don't use it at all.]\n",
    "    N4[\"Prepare latents\" is where we set up the initial noise, ex. generate the noisy image, that we're going to sculpt into something resembling an image.]\n",
    "    N5[\"Classifier-free guidance\" is the standard guidance of SDXLPipeline.__call__ that guides the de-noising to resemble the text prompt]\n",
    "    \n",
    "    %% Positioning with dotted lines\n",
    "    %% A -.- N1\n",
    "    G -.- N2\n",
    "    %% G -.- N3\n",
    "    C -.- N4\n",
    "    F -.- N5\n",
    "    \n",
    "    classDef selfGuided fill:#ffe6cc,stroke:#333,stroke-width:2px;\n",
    "    class G,I selfGuided;\n",
    "    classDef comment fill:#ffe6cc,stroke:#333,stroke-width:1px;\n",
    "    class N1,N2,N3 comment;\n",
    "    %% linkStyle 12,13,14 stroke-width:2px,stroke-dasharray: 5,5;\n",
    "```\n",
    "\n",
    "**SDXLPipeline.__call__(...)**\n",
    "- Only used during inference (hint: you could tell by it only contains de-noising!)\\\n",
    "- `__call__` runs the de-noising for **all** the timesteps to generate the final image (latent)\n",
    "\n",
    "**Update Latents**\n",
    "- The latent space has all variations of the noise images and possible variations of imagery possible\n",
    "  \n",
    "<details>\n",
    "  <summary>Questions</summary>\n",
    "\n",
    "- when is self-guidance active or not active?\n",
    "  - do you run self-guidance multiple times during a single timestep?\n",
    "- explain the steps that are happening in all the orange stuff (self guidance, how does it do what we need? how does the inputs in call guide it to do what we need? e.g. move an object, change background, etc?)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0b6d2-b3ec-44eb-acb8-907543018074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
