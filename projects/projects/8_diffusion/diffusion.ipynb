{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6439b480-6c48-4f75-be6a-aa35eda017e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change parent dir to the \"projects\" folder (../8_diffusion)\n",
    "import sys\n",
    "sys.path.append('..')  # Adds the parent directory to the Python path\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-colorblind\")\n",
    "\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    models,\n",
    "    optimizers,\n",
    "    utils,\n",
    "    callbacks,\n",
    "    metrics,\n",
    "    losses,\n",
    "    activations,\n",
    ")\n",
    "from keras.utils import register_keras_serializable\n",
    "\n",
    "from utils import display, sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba859888-5543-42ec-906c-2260d99806f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "DATASET_REPETITIONS = 5\n",
    "LOAD_MODEL = False\n",
    "\n",
    "NOISE_EMBEDDING_SIZE = 32\n",
    "PLOT_DIFFUSION_STEPS = 20\n",
    "\n",
    "# optimization\n",
    "EMA = 0.999\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956ea29f-456b-490d-bd81-1dd25df7f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory ../data/pytorch-challange-flower-dataset/dataset. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/pytorch-challange-flower-dataset/dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/steven4354/deep-learning-pytorch-book/myenv/lib/python3.12/site-packages/keras/src/utils/image_dataset_utils.py:320\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    316\u001b[0m image_paths, labels \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mget_training_or_validation_split(\n\u001b[1;32m    317\u001b[0m     image_paths, labels, validation_split, subset\n\u001b[1;32m    318\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_paths:\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllowed formats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWLIST_FORMATS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m     )\n\u001b[1;32m    325\u001b[0m dataset \u001b[38;5;241m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[1;32m    326\u001b[0m     image_paths\u001b[38;5;241m=\u001b[39mimage_paths,\n\u001b[1;32m    327\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    339\u001b[0m )\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: No images found in directory ../data/pytorch-challange-flower-dataset/dataset. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_data = utils.image_dataset_from_directory(\n",
    "    \"../data/pytorch-challange-flower-dataset/dataset\",\n",
    "    labels=None,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=None,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    interpolation=\"bilinear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a75bc3-606e-4414-8e17-00ee362e08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(img):\n",
    "    img = tf.cast(img, \"float32\") / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "train = train_data.map(lambda x: preprocess(x))\n",
    "train = train.repeat(DATASET_REPETITIONS)\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e5f37-3094-4fdf-8363-ff2e4143c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some items of clothing from the training set\n",
    "train_sample = sample_batch(train)\n",
    "display(train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ada68-1aca-4145-a932-6dc2b541c676",
   "metadata": {},
   "source": [
    "### 1.1 Diffusion schedules <a name=\"diffusion_schedules\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e4683-f436-44bd-be49-f532df9adc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: define these diffusion schedule's purpose\n",
    "# A: \n",
    "def linear_diffusion_schedule(diffusion_times):\n",
    "    min_rate = 0.0001\n",
    "    max_rate = 0.02\n",
    "    betas = min_rate + diffusion_times * (max_rate - min_rate)\n",
    "    alphas = 1 - betas\n",
    "    alpha_bars = tf.math.cumprod(alphas)\n",
    "    signal_rates = tf.sqrt(alpha_bars)\n",
    "    noise_rates = tf.sqrt(1 - alpha_bars)\n",
    "    return noise_rates, signal_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8afe3-8d67-4452-b78c-45772aaca7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_diffusion_schedule(diffusion_times):\n",
    "    signal_rates = tf.cos(diffusion_times * math.pi / 2)\n",
    "    noise_rates = tf.sin(diffusion_times * math.pi / 2)\n",
    "    return noise_rates, signal_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697007c-6919-4876-9d8f-db99b0710177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_cosine_diffusion_schedule(diffusion_times):\n",
    "    min_signal_rate = 0.02\n",
    "    max_signal_rate = 0.95\n",
    "    start_angle = tf.acos(max_signal_rate)\n",
    "    end_angle = tf.acos(min_signal_rate)\n",
    "\n",
    "    diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "\n",
    "    signal_rates = tf.cos(diffusion_angles)\n",
    "    noise_rates = tf.sin(diffusion_angles)\n",
    "\n",
    "    return noise_rates, signal_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f66a2-f7c6-4872-a62b-e9a6557849fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "diffusion_times = tf.convert_to_tensor([x / T for x in range(T)])\n",
    "linear_noise_rates, linear_signal_rates = linear_diffusion_schedule(\n",
    "    diffusion_times\n",
    ")\n",
    "cosine_noise_rates, cosine_signal_rates = cosine_diffusion_schedule(\n",
    "    diffusion_times\n",
    ")\n",
    "(\n",
    "    offset_cosine_noise_rates,\n",
    "    offset_cosine_signal_rates,\n",
    ") = offset_cosine_diffusion_schedule(diffusion_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5b56c-21c3-4ed3-a5b0-3997d564a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    diffusion_times, linear_signal_rates**2, linewidth=1.5, label=\"linear\"\n",
    ")\n",
    "plt.plot(\n",
    "    diffusion_times, cosine_signal_rates**2, linewidth=1.5, label=\"cosine\"\n",
    ")\n",
    "plt.plot(\n",
    "    diffusion_times,\n",
    "    offset_cosine_signal_rates**2,\n",
    "    linewidth=1.5,\n",
    "    label=\"offset_cosine\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"t/T\", fontsize=12)\n",
    "plt.ylabel(r\"$\\bar{\\alpha_t}$ (signal)\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6de12f-5eeb-48ac-b8e5-37e9f2971900",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    diffusion_times, linear_noise_rates**2, linewidth=1.5, label=\"linear\"\n",
    ")\n",
    "plt.plot(\n",
    "    diffusion_times, cosine_noise_rates**2, linewidth=1.5, label=\"cosine\"\n",
    ")\n",
    "plt.plot(\n",
    "    diffusion_times,\n",
    "    offset_cosine_noise_rates**2,\n",
    "    linewidth=1.5,\n",
    "    label=\"offset_cosine\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"t/T\", fontsize=12)\n",
    "plt.ylabel(r\"$1-\\bar{\\alpha_t}$ (noise)\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a9599-84df-40a7-99dd-d8885585a05c",
   "metadata": {},
   "source": [
    "## 2. Build the model <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3882fb-2602-4ddb-8ddd-ca9bbe91146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: define the purpose of using a sinusoidal embedding vs other embedding variations in diffusion\n",
    "# A: \n",
    "\n",
    "@register_keras_serializable()\n",
    "def sinusoidal_embedding(x):\n",
    "    frequencies = tf.exp(\n",
    "        tf.linspace(\n",
    "            tf.math.log(1.0),\n",
    "            tf.math.log(1000.0),\n",
    "            NOISE_EMBEDDING_SIZE // 2,\n",
    "        )\n",
    "    )\n",
    "    angular_speeds = 2.0 * math.pi * frequencies\n",
    "    embeddings = tf.concat(\n",
    "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5e44d-0999-4242-845e-2df1c5097c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = []\n",
    "for y in np.arange(0, 1, 0.01):\n",
    "    embedding_list.append(sinusoidal_embedding(np.array([[[[y]]]]))[0][0][0])\n",
    "embedding_array = np.array(np.transpose(embedding_list))\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xticks(\n",
    "    np.arange(0, 100, 10), labels=np.round(np.arange(0.0, 1.0, 0.1), 1)\n",
    ")\n",
    "ax.set_ylabel(\"embedding dimension\", fontsize=8)\n",
    "ax.set_xlabel(\"noise variance\", fontsize=8)\n",
    "plt.pcolor(embedding_array, cmap=\"coolwarm\")\n",
    "plt.colorbar(orientation=\"horizontal\", label=\"embedding value\")\n",
    "ax.imshow(embedding_array, interpolation=\"nearest\", origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bef43d-a02d-4264-af29-9dca67350e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResidualBlock(width):\n",
    "    def apply(x):\n",
    "        input_width = x.shape[3]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
    "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=3, padding=\"same\", activation=activations.swish\n",
    "        )(x)\n",
    "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
    "        x = layers.Add()([x, residual])\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def DownBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(width)(x)\n",
    "            skips.append(x)\n",
    "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def UpBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
    "        for _ in range(block_depth):\n",
    "            x = layers.Concatenate()([x, skips.pop()])\n",
    "            x = ResidualBlock(width)(x)\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4e96d-6fb3-4c27-8e79-4b9d954e13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the U-Net\n",
    "\n",
    "noisy_images = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "x = layers.Conv2D(32, kernel_size=1)(noisy_images)\n",
    "\n",
    "noise_variances = layers.Input(shape=(1, 1, 1))\n",
    "noise_embedding = layers.Lambda(sinusoidal_embedding)(noise_variances)\n",
    "noise_embedding = layers.UpSampling2D(size=IMAGE_SIZE, interpolation=\"nearest\")(\n",
    "    noise_embedding\n",
    ")\n",
    "\n",
    "x = layers.Concatenate()([x, noise_embedding])\n",
    "\n",
    "skips = []\n",
    "\n",
    "x = DownBlock(32, block_depth=2)([x, skips])\n",
    "x = DownBlock(64, block_depth=2)([x, skips])\n",
    "x = DownBlock(96, block_depth=2)([x, skips])\n",
    "\n",
    "x = ResidualBlock(128)(x)\n",
    "x = ResidualBlock(128)(x)\n",
    "\n",
    "x = UpBlock(96, block_depth=2)([x, skips])\n",
    "x = UpBlock(64, block_depth=2)([x, skips])\n",
    "x = UpBlock(32, block_depth=2)([x, skips])\n",
    "\n",
    "x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
    "\n",
    "unet = models.Model([noisy_images, noise_variances], x, name=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a55ccb-011d-4110-88ec-88e41cbd58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalizer = layers.Normalization()\n",
    "        self.network = unet\n",
    "        self.ema_network = models.clone_model(self.network)\n",
    "        self.diffusion_schedule = offset_cosine_diffusion_schedule\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.noise_loss_tracker = metrics.Mean(name=\"n_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.noise_loss_tracker]\n",
    "\n",
    "    def denormalize(self, images):\n",
    "        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n",
    "        return tf.clip_by_value(images, 0.0, 1.0)\n",
    "\n",
    "    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n",
    "        if training:\n",
    "            network = self.network\n",
    "        else:\n",
    "            network = self.ema_network\n",
    "        pred_noises = network(\n",
    "            [noisy_images, noise_rates**2], training=training\n",
    "        )\n",
    "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
    "\n",
    "        return pred_noises, pred_images\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
    "        num_images = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "        current_images = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                current_images, noise_rates, signal_rates, training=False\n",
    "            )\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            current_images = (\n",
    "                next_signal_rates * pred_images + next_noise_rates * pred_noises\n",
    "            )\n",
    "        return pred_images\n",
    "\n",
    "    def generate(self, num_images, diffusion_steps, initial_noise=None):\n",
    "        if initial_noise is None:\n",
    "            initial_noise = tf.random.normal(\n",
    "                shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "            )\n",
    "        generated_images = self.reverse_diffusion(\n",
    "            initial_noise, diffusion_steps\n",
    "        )\n",
    "        generated_images = self.denormalize(generated_images)\n",
    "        return generated_images\n",
    "\n",
    "    def train_step(self, images):\n",
    "        images = self.normalizer(images, training=True)\n",
    "        noises = tf.random.normal(shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(BATCH_SIZE, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # train the network to separate noisy images to their components\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, noise_rates, signal_rates, training=True\n",
    "            )\n",
    "\n",
    "            noise_loss = self.loss(noises, pred_noises)  # used for training\n",
    "\n",
    "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "\n",
    "        for weight, ema_weight in zip(\n",
    "            self.network.weights, self.ema_network.weights\n",
    "        ):\n",
    "            ema_weight.assign(EMA * ema_weight + (1 - EMA) * weight)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, images):\n",
    "        images = self.normalizer(images, training=False)\n",
    "        noises = tf.random.normal(shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(BATCH_SIZE, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "        pred_noises, pred_images = self.denoise(\n",
    "            noisy_images, noise_rates, signal_rates, training=False\n",
    "        )\n",
    "        noise_loss = self.loss(noises, pred_noises)\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa746a3a-ef2f-4daf-930a-b23a4f4dfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm = DiffusionModel()\n",
    "ddm.normalizer.adapt(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafca09-b7d0-4863-a5ef-ac9ad287a829",
   "metadata": {},
   "source": [
    "## 3.Train the model <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113e761-2ade-487b-bb80-ddf87a256a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm.compile(\n",
    "    optimizer=optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    ),\n",
    "    loss=losses.MeanAbsoluteError(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b420721-c05a-486f-b354-48be085280bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training and plot generated images periodically\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoints.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "class ImageGenerator(callbacks.Callback):\n",
    "    def __init__(self, num_img):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        generated_images = self.model.generate(\n",
    "            num_images=self.num_img,\n",
    "            diffusion_steps=PLOT_DIFFUSION_STEPS,\n",
    "        ).numpy()\n",
    "        display(\n",
    "            generated_images,\n",
    "            save_to=\"./output/generated_img_%03d.png\" % (epoch),\n",
    "        )\n",
    "\n",
    "\n",
    "image_generator_callback = ImageGenerator(num_img=10)\n",
    "\n",
    "ddm.fit(\n",
    "    train,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        tensorboard_callback,\n",
    "        image_generator_callback,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25567c09-f01a-41c7-9dac-909772495056",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Inference <a name=\"inference\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5eaac5-4396-41d9-b3ac-ca26c12c3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some novel images of flowers\n",
    "generated_images = ddm.generate(num_images=10, diffusion_steps=20).numpy()\n",
    "display(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122ab6e-9861-4158-8981-1ba5edb56af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View improvement over greater number of diffusion steps\n",
    "for diffusion_steps in list(np.arange(1, 6, 1)) + [20] + [100]:\n",
    "    tf.random.set_seed(42)\n",
    "    generated_images = ddm.generate(\n",
    "        num_images=10,\n",
    "        diffusion_steps=diffusion_steps,\n",
    "    ).numpy()\n",
    "    display(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9fc84-89ec-4026-b2dc-1867fe30360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation between two points in the latent space\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "def spherical_interpolation(a, b, t):\n",
    "    return np.sin(t * math.pi / 2) * a + np.cos(t * math.pi / 2) * b\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    a = tf.random.normal(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    b = tf.random.normal(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    initial_noise = np.array(\n",
    "        [spherical_interpolation(a, b, t) for t in np.arange(0, 1.1, 0.1)]\n",
    "    )\n",
    "    generated_images = ddm.generate(\n",
    "        num_images=2, diffusion_steps=20, initial_noise=initial_noise\n",
    "    ).numpy()\n",
    "    display(generated_images, n=11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
